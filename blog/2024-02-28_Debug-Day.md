# Debug Day
First of all, I finally refactored some code, but am still using the notebook from last time to make some quick experiments.

I found a bug from last time where the new implementation of Gaussian expects the scale of the covariance to be comparable to pixels, and not, if you will, of "NDC magnitude".
This I fixed, and went on to handle some other issues with rendering that occurred last time, but now I can render pictures again.

On that note, I observed that 
- When rendering at low resolutions, the pictures seem brighter, presumably because we do not weigh the color that each gaussian contributes by its "average" over a coarse pixel. 
This is a potential thing to fix, which I may or may not get around to.
- We do not need all of the Gaussians to render pictures that look good.
With 10% of the gaussians, I was able to render fairly nice pictures of the splat I have used this far.
This gave me the idea of iteratively refining the picutre - e.g. making a visualizer that first renders a coarse view of the object, before rendering finer details if the user does not change the viewing angle. 
- The stopping criterion when we have saturated a pixel sufficiently seems to not work quite correctly.

I also saw that the IterativeSegmenter from last time does not always find optimal cuts, which needs investigation.
One reason is that if we make a cut and the responsibility on each side of it are equal, then we arbitrarily explore one, and not both of them.
A better algorithm could be keeping track of segments with high responsibility, where we could potentially make progress quickly, but I have to figure out the details later.

The plan for next session is to make rendering photos faster using IterativeImageSegmenter and potentially not using all of the Gaussians.
Potentially, I could also look in to vectorizing some of the computations, as we perform many 3x3 matrix multiplications iteratively, and this is presumably slower than merging those matrices and using parallellism built into numpy.
